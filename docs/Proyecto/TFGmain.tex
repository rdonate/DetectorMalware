%\documentclass[spanish,twoside,12pt]{tfg-esiiab}
\documentclass[spanish,twoside,openright,12pt,a4paper]{book}
\usepackage{tfg-esiiab}

% \title{Manual del paquete \texttt{tfg-esiiab.sty}
%   para la realización en \LaTeXe{} de Trabajos Fin de Grado, según
%   los requisitos de plantilla de la Escuela Superior de
%   Ingeniería Informática de Albacete}
\title{Detección y Clasificación de Malware usando técnicas inteligentes}
\author{Rubén Donate Serrano}

\usepackage{enumitem}

\begin{document}

%De esta forma sale nombre de Tabla en vez de Cuadro
\renewcommand\tablename{Tabla}
\renewcommand\listtablename{ÍNDICE DE TABLAS}

\renewcommand\listfigurename{ÍNDICE DE FIGURAS}

\supervisor{José Luis Martínez Martínez}
\cosupervisor{José Miguel Puerta Callejón}

\maketitle

\frontmatter %Activa la numeración romana

\begin{dedicatoria} %Inclusión de la dedicatoria
La dedicatoria que quiera hacer.
\end{dedicatoria}


\begin{autoria}

Yo, Rubén Donate Serrano con DNI 70519349S, declaro que soy el único autor del trabajo fin de grado titulado Detección y Clasificación de Malware usando técnicas inteligentes y que el citado trabajo no infringe las leyes en vigor sobre propiedad intelectual y que todo el material no original contenido en dicho trabajo está apropiadamente atribuido a sus legítimos autores.

\vspace{2em}

Albacete, a \today

\vspace{2em}

Fdo.: Rubén Donate Serrano
\end{autoria}



\begin{resumen} %%Resumen del documento
Este sería el resumen del TFG.
\end{resumen}

\begin{agradece} %%Agradecimientos
Agradezco el apoyo de mi familia amigos y de mis tutores.
\end{agradece}


\tableofcontents

\clearpage
\listoffigures
\addcontentsline{toc}{section}{Lista de Figuras} 
\clearpage
\listoftables
\addcontentsline{toc}{section}{Lista de Tablas} 
\thispagestyle{empty}\cleardoublepage


\mainmatter %Activa la numeración arábica.

\chapter{INTRODUCCIÓN}


\section{Motivación}


\section{Objetivos}

\section{Estructura de la memoria}

\chapter{TÉCNICAS UTILIZADAS}

\begin{enumerate}
	\item ngram\label{ngram} 
	\item sklearn \label{sklearn}
	\begin{enumerate}
		\item randomForrestClassifier \label{randomForrestClassifier}
		\item DictVectorizer \label{DictVectorizer}
		\item TF-IDF \label{TF-IDF} \cite{montiel2009comparacion}
		\item NMF \label{NMF} \cite{lee1999learning} \cite{cobo2006tecnicas} 
		\item Linear Support Vector Classification \ref{lib_linear_classification} \label{LSVC} 
		\item KFold \label{KFold}
	\end{enumerate}

	\item numpy \ref{numpy} \label{numpy}
	\begin{enumerate}
		\item Concatenate \label{concatenate}
	\end{enumerate}
	\item pickle \label{pickle}
	\item cPickle \label{cPickle}
	\item glob \label{glob}
	\item subprocesss \label{subprocess}
	\item funciones linux:
	\begin{enumerate}
		\item echo \label{echo}
		\item cat \label{cat}
		\item wc \label{wc}
		\item grep \label{grep}
	\end{enumerate}
	\item pandas \label{pandas} DataFrame \label{dataframe}
	\begin{enumerate}
		\item read\_csv \label{read_csv}
		\item concat \label{concat}
		\item to\_csv \label{to_csv}
	\end{enumerate}
	\item Matriz Dispersa \label{sparse}
	\begin{enumerate}
		\item csrMatrix \label{csrMatrix}
	\end{enumerate}
	\item os \label{os}
	\begin{enumerate}
		\item path \label{os_path}
	\end{enumerate}
	\item joblib \label{joblib}
	\begin{enumerate}
		\item parallel \label{parallel}
		\item delayed \label{delayed}
	\end{enumerate}
	\item LibLinear: Una librería para completa clasificación lineal \label{lib_linear_classification}  \cite{fan2008liblinear}
	\item hickle \label{hickle}
	\item XGBoost \label{xgboost} \cite{xgboost_parameters}
	\item Gradient Boost Tree \label{gbtree}
	\item logloss \label{logloss}
	\item Dual Optimization Problem \label{dual_Optimization_Problem} \cite{singer1986general}
	\item Random \label{random}
	\begin{enumerate}
		\item Sample\label{random_sample} 
		\item Choice \label{random_choice}
	\end{enumerate}
	\item Bagging \label{bagging}
	\item Scipy \label{scipy}
	\begin{enumerate}
		\item Stats \label{scipy_stats}
		\begin{enumerate}
			\item rv\_discrete \label{rv_discrete}
		\end{enumerate}
	\end{enumerate}
	\item MongoDB \label{mongodb}
	\item NoSQL \label{nosql}
	\item JSON \label{json}
	\item BSON \label{bson}
	\item Docker \label{docker}
	\item re \label{re}
\end{enumerate}


\chapter{ANTECEDENTES Y ESTADO DE LA CUESTIÓN}
%\chapter{Antecedentes y estado estado de la cuestión cuestión cuestión cuestión}
Para el proyecto de Kaggle que se ha seleccionado para este Trabajo de Final de Grado, se he realizado una búsqueda para intentar encontrar soluciones presentadas para este reto. De esta búsqueda, se ha conseguido encontrar varias soluciones presentadas para este mismo reto. A continuación, se explicaran las soluciones presentadas que se han encontrado para este reto de Kaggle.\\

\section{Solución 1} \label{solution1}

Esta primera solución que se va a comentar, fue realizada por Vishnu Chevli \cite{VishnuChevli} y en la clasificación del Reto de Kaggle obtuvo una resultado en el ranking publico de $0.023121984$ en la posición 90 y en el ranking privado $0.018856579$ en la posición 72.\\

Esta solución se desarrollo para que pudiera ser ejecutada de manera indistinta en Python 2 y Python 3, es muy sencilla y consiste en:\\

En primero lugar, se descomprimir las dos bases de datos que se proporcionan. Estas bases de datos contienen dos tipos de ficheros que contienen la información obtenida de IDA al desensamblar la muestras en dos formatos, siendo estos formatos ASM para los ficheros con extensión .asm y binario en formato hexadecimal para los ficheros con extensión .bytes. Para esta solución solo se utilizaran los ficheros con extensión .bytes. Después, estos ficheros son nuevamente comprimidos en formato gzip de manera separada y separando las bases de datos en dos carpetas distintas. El motivo de realizar esto es para ahorrar espacio en disco, ya que el espacio de la base de datos completas con todos los tipos de ficheros sin comprimir es de aproximadamente 1 Tb.\\

El siguiente paso, consiste en extraer las características de los ficheros que se han generado en el paso anterior. Esta extracción consiste en generar un array donde cada posición corresponde a cada uno de los posibles uno ngrama \ref{ngram}, formado por dos caracteres en formato hexadecimal, incluyendo el ngram '??'\ que indica que el valor correspondiente no esta mapeado, y contar las veces que aparece cada uno de ellos en el fichero, para concluir, guardando los resultados en un fichero csv de manera separada para cada una de las bases de datos. Este proceso ser realizara de manera conjunta para las dos bases de datos y de manera paralela para 2 ficheros.\\

El siguiente paso, es construir el modelo que para este caso es un RandomForrestClassifier \ref{randomForrestClassifier}. Para ello, lo primero se tiene que realizar es obtener la clase para cada uno de los fichero, para lo cual se tiene que abrir y recorrer el fichero 'trainLabels.csv'\ que se proporciona y almacenar la información del mismo en un diccionario que posteriormente se utilizará. A continuar, se crea una matriz con numpy \ref{numpy} de tamaño el numero de ficheros de la base de datos de train por el número de uno ngramas \ref{ngram} mas uno adicional para la clase, en este caso $10868 * 258$. Estos datos son obtenidos del fichero csv correspondiente a la base de datos de train para los valores de los uno ngramas \ref{ngram} y del diccionario creado anteriormente para la clase.  Después, se crea el modelo con los valores por defecto, excepto la semilla que utiliza $123$, el número de hilos que utiliza $5$ y el nivel de información que muestra que utiliza la mas detallada $2$. Por ultimo, solo queda entrenar el modelo pasando le la matriz de numpy \ref{numpy} separando las características y la clase, o lo que es lo mismo la matriz sin la ultima columna y la ultima columna por separado, siendo esta ultima columna la clase.\\

Para terminar, solo nos queda realizar la previsión para la base de datos de test. Para ello, ahí que repetir el proceso de recuperar las características creando una matriz con numpy \ref{numpy} pero en este caso sin añadir un columna para la clase, también se crea un array donde se almacena el nombre del fichero en el mismo indice que la fila de la matriz, esto es así porque la predicción ha de ser identificada con es valor. Lo siguiente, es realizar la predicción para la base de datos de test. Para concluir, escribiendo los resultados en un fichero comprimido en el que guarda la información con el formato que se nos proporciona en el ejemplo, siendo este el id del fichero seguido de la probabilidad que sea cada una de las clases separado todo por comas.\\

\section{Solución 2} \label{solution2} 

Esta segunda solución que se va a comentar, fue realizada de manera conjunta por Mikhail Trofimov, Dmitry Ulyanov y Stanislav Semenov \cite{MikhailTrofimovDmitryUlyanovStanislavSemenov} y en la clasificación del Reto de Kaggle obtuvo una resultado en el ranking publico de $0.005984430$ en la posición 14 y en el ranking privado $0.003969846$ en la posición 3.\\

Esta solución es mucho mas compleja como se puede suponer de la posición que obtuvo en el ranking, de hecho según sus creadores utilizaron una maquina con 16 cores y 120 Gb de RAM para su ejecución. Ahora, se analizará en que consiste esta solución:\\

Lo primero que realizar es crear los directorios, para ello existe un script en bash llamado 'create\_dirs.sh'\ para generar los directorios necesarios. A continuación, toca ejecutar otro script en bash llamado 'main.sh'\ para ejecutar esta solución, siendo el primer paso, la extracción de las características de los ficheros con extensión '.asm'\ que posteriormente se van a utilizar para construir el modelo. Pero antes de eso, ahí que establecer en el fichero llamado 'set\_up.py'\ los parámetros fijos para esta solución en variables, como pueden ser el número de hilos, y las distintas direcciones de las carpetas con las que trabaja la solución. A continuación, se comentará este proceso de extracción que consisten en:\\

El primer paso en el proceso de extracción de características de los ficheros con extensión '.asm', consiste en contar las veces que aparece cada una de las secciones en cada uno de las muestras de las bases de datos y la suma total del número de ocurrencias de cada sección para cada muestra. Para llevar acabo esto, se recorre cada fichero con extensión '.asm', siendo pasado el nombre de éste por parámetro a la función, y de cada linea, se tiene que coger la primera palabra, estando esta delimitada con dos punto, y se mantiene en un diccionario el conteo de ocurrencias que aparece cada una de las secciones y el número total de ocurrencias de cada sección. Para proporcionar persistencia y no tener que almacenar toda la información en memoria, se guarda en un archivo serializado con el paquete pickle \ref{pickle} el diccionario obtenido para ese fichero. Para obtener el nombre del fichero que se le pasa para hacer la extracción, se consigue la dirección completa de los ficheros con extensión '.asm'\ en cada una de las carpetas donde se almacenan las bases de datos, usando para esto la función glob del paquete glob \ref{glob}. Posteriormente, a los elementos de estas lista se les aplica una función map para dejar solo el nombre del fichero que es el identificador la muestra. Para terminar, se crea una lista de procesos cuya longitud es el número de hilos establecidos. Esto es así para realizar el procesado de las muestras de manera paralela. A estos procesos, se les pasa una sublista de los ficheros y ejecuta una función worker que se encarga de procesar cada uno de los elementos de esa sublista y realizar la extracción comentada anteriormente. La manera de formar estas sublista es mediante el resto del indice de la posición en la lista original entre el número de hilos que se ejecutan, siendo este el indice del proceso en el cual la muestra tiene que formar parte de su sublista.\\

El segundo paso en el proceso de extracción de características en los ficheros con extensión '.asm', consiste en llevar un conteo de las lineas que posee el fichero de la muestra y en cuantas de estas, aparece los elementos de un conjunto establecido de instrucciones asm. Este conjunto de instrucciones que se utiliza en la solución, se puede observar en la figura  \ref{ListaInstruccionesASM}, está formado por 23 instrucciones ASM, siendo estas las mas habituales, almacenadas en una lista de strings.\\

\begin{figure}[hbtp]
	\centering
	\includegraphics[width=0.75\textwidth]{ListaInstruccionesASM.png}
	\caption{Lista de instrucciones ASM manejadas.}
	\label{ListaInstruccionesASM}
\end{figure}

Se comienza por almacenar en el fichero llamado 'fnames'\ el nombre de la muestra, usando la función echo \ref{echo} de linux, que se le pasa a la función, siendo este nombre generado de la misma manera que en le paso anterior. Realizarlo de está manera implica que no sea posible la paralelización de este proceso, ya que la linea dentro de cada fichero que se va a utilizar estable la muestra a la que se hace mención. Se continua realizando el conteo de la lineas del fichero con extension '.asm'\ de la muestra. Para llevar acaba el proceso de conteo, se utiliza la función call del paquete subprocess \ref{subprocess} para de esa manera, utilizar las utilidades de linux. En este caso, para contar el número total de lineas del fichero de muestra se utiliza la función cat \ref{cat} para recorrer el y se le pasa esa información mediante una tubería a la utilidad wc \ref{wc} con el parametro '-l', la cual cuenta la lineas que se le han pasado y añade esa información en un fichero llamado 'line\_count'\ en la posición correspondiente para esa muestra. A continuación, se realizará el conteo de la lineas en las que aparece cada una de las instrucciones en esa muestra. Para conseguir este objetivo, hace uso de la función grep \ref{grep} para seleccionar las lineas dentro de la muestra que contiene la instrucción correspondiente, para posteriormente pasárselo mediante una tubería a la función wc \ref{wc}, igual que en el paso anterior, y añadir la información en el fichero correspondiente a la instrucción en la posición que le corresponde. Este paso se repite, para cada una de las instrucciones establecida. En este proceso, se realizar de manera serizalizada para todos los ficheros de la base de datos y en el caso de ser el primer fichero y no existir los ficheros todos ellos son generados cuando se añade el primer elemento.\\

El siguiente paso en el proceso de extracción de características en los ficheros con extensión '.asm', consiste en seleccionar de la muestra las lineas en las que aparece la cadena de caracteres '\_\_stdcall'\ y almacenarlas en un fichero por cada muestra. Para llevar esto acabo, hace uso de la función call del paquete subprocess \ref{subprocess} y de la función grep \ref{grep}, igual que en el paso anterior, pero en esta ocasión se almacena por nombre de la muestra, la cual se le pasa a la función del mismo modo que en los pasos anterior. Esto implica que en este caso si sea posible paralelizar el proceso de extracción de características, por lo cual se realiza de la misma manera que se llevo acabo en el primer paso, creando una sublista que procesara un proceso para todos las muestras de las bases de datos.\\

Por ultimo en el proceso de extracción de características en los ficheros con extensión '.asm', consiste en seleccionar de la muestras las lineas en la que aparece la cadena de caracteres 'FUNCTION'. Como se puede observar, consiste en repetir el proceso anterior una nueva cadena de caracteres.\\

Una vez terminado el proceso de extracción de características de los ficheros con extensión '.asm', toca preprocesar la información obtenida de los ficheros con extensión '.asm'. Para ello, lo primero que se realiza es obtener dos dataframes a partir los ficheros 'trainLabel.csv'\ y 'sampleSubmission.csv', los cuales son proporcionados en el reto, con pandas \ref{pandas}. Éstos contienen los identificadores junto con otro información de las muestras de las dos bases de datos train y test respectivamente, y se usarán para que durante el proceso de preprocesado todas las características obtenidas de la muestras guarden el mismo orden, hecho por el cual esto implica que este procedimiento no puede ser paralelizado. Además, los procesos que se comentara a continuación se repite para estos dos datafame. Este procedimiento consiste en:\\

En primer lugar, se trataran las secciones obtenidas, para ello se recorre el dataframe fila a fila, y para el campo Id se abre el fichero de las secciones calculado correspondiente con el paquete cPickle \ref{cPickle}, el cual se trata de un diccionario en python, por lo cual se recorre y si pertenece a un conjunto establecido, el cual es puede observar en la figura \ref{ListSectionsWhitelistASM}, se incluye  en otro diccionario, para terminar guardándolo en la posición del array que viene establecida por el dataframe. En este primer paso, cuando se trata del dataframe de la base datos de train también se crea un array, en el cual se almacena la etiqueta correspondiente de la muestra en el indice establecido por el dataframe. Para continuar, transformando el array de diccionarios en una matriz, para esto usa el modelo DictVectorizer \ref{DictVectorizer} del paquete sklearn \ref{sklearn} con el parámetro sparse a false para que no almacenar los datos en una matriz dispersa \ref{sparse}. Este modelo se entran con el array generado con el dataframe de la base de datos de train y posteriormente se transforman los dos array en su correspondiente matriz. Con estas matrices se procede a calcular la entropía correspondiente a cada una de ellas. Terminando así el tratamiento de las secciones.\\

\begin{figure}[hbtp]
	\centering
	\includegraphics[width=0.75\textwidth]{ListSectionsWhitelistASM.png}
	\caption{Lista de secciones ASM manejadas.}
	\label{ListSectionsWhitelistASM}
\end{figure}

En segundo lugar, consiste en obtener el tamaño de los archivos con extension '.asm'\ y '.bytes'\ para después obtener también el ratio entre estos. De este modo, se crea una matriz con la longitud del dataframe y dos columnas una para cada fichero, donde se almacena con la ayuda de la función getsize del paquete os.path \ref{os_path} el tamaño estos correspondientes a la muestra en la posición establecida por el dataframe. A partir de esta matriz, se genera otra nueva con una sola columna, la cual es el resultado de hacer la división de float, ya que para garantizarlo se multiplica en numerador por $1,0$,del tamaño obtenido del fichero con extensión '.bytes'\ entre el tamaño obtenido del fichero con extensión '.asm', concluyendo así este paso.\\

En tercer lugar, consisten en almacenar la información obtenida de las instrucciones asm en una matriz de numpy \ref{numpy} y el calculo de la entropía de la misma. Para ello, primero se tiene que abrir y recorrer el fichero 'fnames'\ que se genero en el proceso de extracción de características para almacenar en un diccionario donde se almacena en el valor de indice la muestra se almacena un diccionario con los valores obtenidos para las instrucciones manejadas, vease la figura \ref{ListaInstruccionesASM}, para esa muestra. Una vez se recuperado la información, se procede a almacenar la en una matriz de numpy \ref{numpy} conforme se va haciendo, donde el indice que el dataframe establece la fila y en indice del array de figura \ref{ListaInstruccionesASM} indica la columna. Una vez realizado esto se procede se calcula la entropía de cada característica. Este proceso se repite para los dos dataframe y con esto se termina este paso.\\

El siguiente paso, consiste recuperar la información del número de lineas de los ficheros con extensión '.asm'\ y almacenarlo en un diccionario. Esto se realiza de este modo debido a que se tiene que recorrer el fichero 'fnames'\ para obtener el Id de valor almacenado en el fichero 'line\_count'. Una vez hemos recuperado la información se  procede a almacenarla en una matriz con una sola columna en el orden establecido y por el dataframe y se calcula la entropía para todos los valores de la misma. Este proceso se repite de igual manera para los dos dataframe, terminando lo así.\\

A continuación, toca obtener la información de las llamadas a las funciones, estando estas en las lineas seleccionadas en las que aparecía la cadena '\_\_stdcall'\ almacenadas en el proceso anterior. Para esto, se procede a recorrer los ficheros obtenidos anteriormente en el orden establecido por el dataframe y en un primer paso, partir esa linea para obtener el nombre de la función para a continuación concatenar los y almacenarlos en un string separadas por espacios para cada muestra, el cual es almacenada en un array en la posición establecida por el dataframe. Una vez obtenido este array para los dos dataframe, se procede a crear un TfidfVectorizer \ref{TF-IDF} mediante el cual se obtienen como máximo las $10000$ características mas significativas según este modelo. Para ello, se entrena el modelo con la unión de unión de los  dos array para posteriormente, transformar estos una matriz dispersa \ref{sparse} con la probabilidad de como máximo las $10000$ características mas significativas para cada una de las muestras. Una vez obtenida esta matriz, se le realiza una factorización de matriz no negativa \ref{NMF}, para de ese modo concluir obteniendo las $10$ características mas representativas representativas. Por lo cual, se construye en modelo NMF para 10 componentes y se entrena con una matriz dispersa \ref{sparse}, consistente en la unión de las dos matriz obtenidas en el paso anterior, para concluir transformado cada una de de las matrices y así obtener una matriz dispersa con la probabilidad factorizada de las características de las dos bases de datos.\\

Para terminar con el proceso de prepocesado de los fichero con extensión '.asm', solo nos queda tratar las llamadas al sistema, siendo las lineas seleccionados en las que aparecen la cadena 'FUNCTION'. Este proceso es idéntico al llevado a cabo en el paso anterior salvo que se cogen los nombres de las funciones en la que en la linea aparece la palabra 'PRESS'.\\

Para concluir, se almacena en una matriz de numpy \ref{numpy} donde se incluye en columnas la información preprocesada de la secciones, el ratio del tamaño, las instrucciones, el tamaño de los ficheros, la entropía de las secciones, las características obtenidas de las llamadas a las funciones y por ultimo las características de las llamadas la sistema. Este proceso se repite de igual manera para las dos bases de datos. Por ultimo se almacena en un fichero serializado con joblib \ref{joblib} llamado 'X\_basepack'\ guardando las dos matrices en una tupla. Con esto se termina el proceso de preprocesado de las características de los ficheros con extensión '.asm'.\\

Llegados a este punto, el siguiente paso consiste en extraer las características de los ficheros con extensión '.bytes'. Este paso comienza por, crear un diccionario mediante el cual traducir un n-grama de dos caracteres en hexadecimal a decimal. A continuación, se abre el fichero de la muestra y cada linea se transforma en un array teniendo como delimitador el carácter espacio en blanco '\ '. De los arrays generados, se selecciona todos los elementos salvo en primero, que corresponde con la dirección en memoria, para a continuación transformar a decimal los n-gramas con el diccionario generado en le primer paso, y concatenar todos en un nuevo array donde se encuentras todos los n-gramas en formato decimal del fichero ordenados. Una vez tenemos este ultimo array, lo recorremos en grupos de 4 n-gramas, para continuar por transformar el 4 n-grama en un indice número y guardar en un diccionario las ocurrencias de estos indices en el fichero. Para concluir, transformando la información almacenada en diccionario en dos array, uno para los indices y otro para las ocurrencias del mismo. Para ello, recorre las keys del diccionario y almacena en la misma posición en ambos array el indice y el valor de este, respectivamente. Para terminar, almacena en un archivo serializado con pickle \ref{pickle} un tupla con estos dos array. Este proceso es llevado acabo para las dos bases de datos. Para ello, se obtiene los ficheros que los ficheros en la ubicación de la base de datos con la función glob de glob \ref{glob}. Un vez tenemos la lista de ficheros de la base de datos, se procesan de manera paralela todos ellos haciendo uso de todos las CPU disponibles utilizando la función parallel \ref{parallel} del paquete joblib \ref{joblib}.\\

En este momento, se comienza con el preprocesado de la información de los ficheros con extensión '.bytes', siendo este primer paso contar las veces que aparece cada n-grama representado por un indice en formato decimal en los primeros $4000$ ficheros. Para ello, se obtienen los ficheros de características obtenidas en el paso anterior con la función glob del paquete glob \ref{glob}. A continuación, se genera un array de numpy \ref{numpy} de ceros con tamaño el número de 4 n-gramas posibles, o lo que es lo mismo $257^{4}$. Para continuar, obtener la información obtenida para a continuación sumar uno en el array de numpy \ref{numpy} generado en los indices que posee la muestra. Una vez, procesados $4000$ de estos ficheros se termina guardando los resultados en un archivo serializado con la función pickle del paquete cPickle \ref{cPickle}.\\

El segundo paso que se realizar en este preprocesado, de la información conseguida del paso anterior selecciona la posición de los que hayan obtenido un valor superior a 10 y se crea un diccionario donde se almacena el indice original y su posición en el nuevo posición. Con esta información, el siguiente paso consiste en modificar las características obtenidas seleccionando solo las que forman parte del diccionario que se acaba de obtener. Para ello, se recuperan las características obtenidas, recorriendolas y si pretenecen al diccionario de características frecuentes se incluye en los nuevos array, ya que se almacena mediante dos array uno primero para almacenarlas características y otro segundo para almacenar el valor de esa característica en la misma posición, del mismo modo que se almaceno en un primero momento. Este proceso se realiza para todos los archivos que se han generado en la extracción de características. Además, se paraleliza con la función parallel \ref{parallel} del paquete joblib \ref{joblib} para $15$ de los $16$ CPUs disponibles.\\

Por ultimo, se procede a recuperar las características y almacenarlas en una matriz dispersa \ref{sparse}, para a continuación pasársela al modelo Linear Support Vector Classification \ref{LSVC} para reducir los outliers que hayan detectado. Para ello, en primer lugar se recuperan los datos en el formato correcto, por lo cual se abre el archivo 'trainLabels.csv'\ correspondiente a la base de datos de train, el cualse proporciona. Llevando a cabo esto con la función read\_csv del paquete pandas \ref{pandas}, obteniendo un dataframe, el cual recorreremos y obtenemos los valores que se han generado en el paso anterior. Estos datos están formados por los valores los cuales se incluyen en un array llamado 'data', los indices de los n-gramas que se incluyen en un array llamado 'indices'\ y por ultimo en un array llamado 'ptr'\ donde se almacena el indice donde empieza cada una de los valores de cada una de las muestra, para conseguir esto, se acumula la longitud de los valores recuperados en una variable llamada 'cur\_bound'\ y se añade el valor de esta variable en cada momento en el susodicho array. Una vez formados estos arrays se guarda en una tupla formada por arrays de numpy \ref{numpy} de los tres array obtenidos en un archivo serializado con el paquete hickle \ref{hickle} y a continuación, se crea una matriz dispersa con la función csr\_matrix \ref{csrMatrix} del paquete \ref{sparse}, la cual también es guardada en un archivo serializado con el paquete joblib \ref{joblib}.\\

A continuación, se repite el proceso anterior para la base de datos de test, por lo cual se genera un dataframe a partir del fichero 'sampleSubmission.csv'\ proporcionado con la función read\_csv de paquete pandas \ref{pandas} y repitiendo con este el proceso de recuperación y almacenaje en una matriz dispersa de las características comentado en el paso anterior.\\

Llegado a este punto, ahí que construir y entrenar el modelo Linear Support Vector Classification \ref{LSVC}, por lo cual, se recuperar la matriz dispersa de las características generada en el paso anterior para la base de datos de train, mediante el paquete joblib \ref{joblib}. También se obtiene de nuevo el dataframe de la base de datos de train, mediante la función read\_csv del paquete pandas \ref{pandas} leyendo el fichero 'trainLabels.csv'\ proporcionado. A continuación se crea el modelo Linear Support Vector Classification \ref{LSVC}, con los siguientes parámetros penality=l1m con la cual se establece que la penalización utiliza en el modelo sea la 'l1'\ del modulo liblinear \ref{lib_linear_classification}, max\_iter=20 con el que se establece el número máximo de iteracciones que va ha realizar el modelo, dual=False con la cual se establece que se utiliza el problema de optimización dual \ref{dual_Optimization_Problem} y verbose=1 con el que se establece que el nivel de detalle de información que se muestra mientras se crea el modelo siendo que no muestre información. Para después, entrenar el modelo con la matriz dispersa y las etiquetas correspondientes a las muestras, obtenidas del dataframe y guardando el modelo una vez entrenado en un archivo serializado con el paquete \ref{joblib}. Para terminar, transformando las matriz a partir del modelo aprendido eliminar los outliers, en caso de que la matriz no se haya recuperado, en el caso de la matriz dispersa de las características de test, primero se obtiene la misma mediante el paquete joblib \ref{joblib}. Además, las dos nuevas matrices son guardadas en un archivo serializado con le paquete joblib \ref{joblib}, terminando de este modo el preprocesado de las características.\\

El siguiente paso en esta solución, es reducir la dimensionalidad de las características de los ficheros con extensión '.bytes'. Para ello, se recuperan los datos preprocesados en el paso anterior para las base de datos de train y de test y transformando la matriz dispersa en una matriz, también se obtiene mediante la función read\_csv del paquete pandas \ref{pandas} el dataframe de la base de datos de train del archivo 'trainLabels.csv'\ proporcionado. A continuación, se parte la base de datos de train almacenada en la matriz y la etiquetas correspondientes en dos grupos, uno de train y otro de test de un 70 \% y 30 \% respectivamente. Estos dos grupos se subdividen en características representada con un X al principio y etiquetas representada con un y al principio, quedando almacenadas en las siguientes variables respectivamente, X\_train, X\_test, y\_train, y\_test. El siguiente paso, consiste en construir el modelo Random Forest Classifier \ref{randomForrestClassifier} con los parámetros n\_jobs=-1 que indica que se usen todas la CPUs y n\_estimators=1000 que establece el número de árboles de decisión que formaran el modelo y entrenándolo con las variables X\_train y y\_train. A continuación, de la característica features\_importances\_ del modelo se seleccionan las que tengan un valor superior a $0,0014$. Para terminar, construyendo una tupla formada por las dos matrices de los datos preprocesado y que formen parte de las características seleccionadas, terminando por guardar esta tupla en un archivo serializado con el paquete cPickle \ref{cPickle}. Terminando así el proceso de selección de variables de los archivos con extensión '.bytes'.\\

En este punto, es cuando se comienza a la construcción de los modelos necesarios para realizar la predicción en el reto. Este proceso se puede dividir en 4, siendo estos, la creación un primer modelo base, seguido de otros dos modelos que serán la base de la predicción final y por ultimo, una ponderación de los resultados obtenidos por estos dos últimos modelos. A continuación se explicará cada uno de pasos.\\

El primero paso, conforme se ha indicado es construir un modelo inicial que servirá de apoyo en para la predicción posterior de otro modelo. Para ello, se comienza por obtener los datos generados tras sus correspondientes preprocesados y selección de variables de los ficheros con extensión '.asm'\ y '.bytes', para a continuación, juntar los en una matriz llamada 'Xtrain' y 'Xtest' respectivamente para la base de datos de train y test, haciendo uso de la función hpstack  del paquete numpy, esto es posible ya que todas las informaciones han sido almacenadas según las posiciones establecidas por el dataframe correspondiente a cada base de datos. También se obtiene la etiqueta de clase de las muestras pertenecientes a la base de datos de train, para ello se obtiene el valor de las clase del dataframe del fichero 'trainLabels.csv'\ proporcionado con la función read\_csv \ref{read_csv} del paquete pandas \ref{pandas} para almacenarlo en una array de numpy \ref{numpy} restándole 1 para las etiquetan comiencen en el valor 0 y guardándolas en una variable llamada 'ytrain'.\\

Una vez contamos con los datos necesarios el siguiente paso consiste en establecer los parámetros necesarios y construir el modelo, siendo este un XGBoost \ref{xgboost}. Los parámetros son guardados en un diccionario transformar los item que la forman en una lista almacenándolo en una variable llamada plst. Los parámetros que se establecen son booster=gbtree con el que se establece que el modelo boost que se creará sea un Gradient Boost Tree \ref{gbtree}, objective=multi:softprob con el que se establece que el resultado sera una matriz donde para cada elemento se mostrara la probabilidad de cada una de las posibles etiquetas de clase, num\_class=9 que establece  que el número de etiquetas para clasificar la clase ese de 9 según se establece en el reto, eval\_metric=logloss con el cual se establece que la métrica a utilizar es logloss \ref{logloss}, scale\_pos\_weight=1.0 con el que se estable que el control del balance de los pesos positivos y negativos que se utiliza habitualmente para desequilibrar la clase \ref{xgboost_parameters}, bst:eta=0.3 con el cual se establece un peso en tanto por 1, en este caso 0,3, con el que se ponderan los pesos que va generan durante la construcción del modelo para establecer la importancia del modelo construido según su acierto a la hora de predecir las muestras proporcionadas al modelo, con el valor que se le ha proporcionado hace que el comportamiento del modelo sea conservador teniendo mas importancia las características ya aprendidas que las nuevas características, bst:max\_depth=6 con el cual se establece la profundidad máxima de los árboles que construirá el modelo, bst:colsample\_bytree=0.5 con el que se establece el porcentaje en tanto por 1 el número de características que van a ser seleccionadas para construir el árbol de decisión en cada iteracción, silent=1 con el cual se establece que no se muestre información durante la generación del modelo y nthread=16 con el que se establece el número de hilos que se usarán para generar el modelo. Además, aparte de los parámetros establecidos en el diccionario también se establecen una variable llamada num\_round=100 que establece el número de rondas que realiza el modelo y una lista vacía llamado watchlist que establece que no se revisa los resultados del modelo con un conjunto de muestras que no es utilizado en el entrenamiento.\\

Llegado a este punto, se genera un array con los indices de las muestras y una matriz de numpy \ref{numpy} de ceros con tamaño el número de muestras a clasificar por el número de clases posibles, en este caso 9. Para continuar creando 20 modelos, comenzando por asignar al diccionario de parámetros el valor de la semilla 'seed', siendo este el número del modelo añadiéndole uno más, y transformando los elementos en un lista para pasárselo al modelo. A continuación, se genera un nuevo array que corresponderá a los indices de las muestras de la base de datos de training que se van a utilizar en el modelo. Para lo cual, lo primero se genera una permutación del array de indices de las muestras que se ha generación al inicio de este punto, haciendo uso de la función sample \ref{random_sample} del paquete random \ref{random}, para a continuación, añadir de manera aleatoria indices del array permutado hasta dejar un array de 8 veces el tamaño de la base de datos.\\

El siguiente paso, es crear dos DMatrix, una para cada base de datos, que es utilizada por pasar al modelo XGBoost \ref{xgboost} los datos. La DMatrix de la base de datos de Train se construye seleccionado los valores de las características de la matriz 'Xtrain'\ y de la etiquetas de 'ytrain'\ de esa base de datos con los indices creados en el paso anterior y en el caso de la base de datos de test, la matriz  DMatrix se construye únicamente pasandole los valores  de 'Xtest'. A continuación, se construye el modelo XGBoost \ref{xgboost} propiamente dicho, para lo cual se le pasan los parametros en forma de lista la Matriz DMatrix correspondiente a la base de datos de train, las etiquetas de los indices seleccionados, el numero de rondas establecido en la variable num\_roound, y los valores con los que se comprobará establecido en la variable watchlist.\\

A continuación, toca realizar la predicción de este modelo para los valores de la base de datos de test almacenados en la DMatrix y reordenarlos para que se ajusten a la matrix de resultados que se genero, donde se acumularan los resultados de todas la predicicones, para a continuación, obtener la media de todos los modelos generados en este primer proceso.\\

Los resultados obtenidos serán guardados en un fichero llamado 'release0.csv', para lo cual se abre el fichero 'sampleSubmission.csv' proporcionado para este reto, con la función read\_csv \ref{read_csv} del paquete pandas \ref{pandas}. Para a continuación, asignar los resultados obtenidos y terminar guardándolo en el fichero ya indicado con la ayuda de la función to\_csv \ref{to_csv} del paquete pandas \ref{pandas} terminando así este primer modelo, siendo este una implementación propia de  un algoritmo de Bagging \ref{bagging} donde el modelo base es un XGBoost \ref{xgboost}.\\

El siguiente paso, consiste en construir el primero de los modelos que se utilizarán para obtener la predicción. Para lo cual, se obtienen los datos obtenidos de los ficheros '.bytes'\ de los ngramas \ref{ngram} preprocesados pero sin reducir, los cuales son dos matrices dispersas por lo cual son transformadas en dos arrays, los cuales se llaman Xtrain y Xtest respectivamente con la bases de datos de train y test. También se recupera la etiqueta de la clase del mismo modo que se realizó en le modelo anterior. También los parámetros son los mismos que en el modelo anterior, con la salvedad que el número de rondas ahora es 150 en lugar de los 100 que se utilizo en le modelo anterior. La construcción del modelo también es muy parecida siendo todo igual salvo que no se amplia la base de datos de train para construir el modelo. Para concluir, se guardan los resultados de la predicción en un fichero 'release0.5.csv' de la misma manera que en el modelo anterior, terminando así este modelo.\\

Ahora, solo queda construir el ultimo modelo, para  lo cual, se recuperan los mismo datos y de la misma manera que en el primero de los modelos. Además, también se recupera la predicción realizada por dicho modelo, para realizar esto se hace uso de la función read\_csv \ref{read_csv} del paquete pandas \ref{pandas}. El siguiente paso, consiste en array de numpy \ref{numpy} según corresponda respectivamente a la etiquetas de la clase y el resto de los datos, quitando si aparece el nombre de la muestra. Se continua, creando un array donde se almacena distribuciones de problabilidad para cada una de la muestras de la base de datos de test, usando para ello la función rv\_discrete \ref{rv_discrete} del paquete stats \ref{scipy_stats} dentro de scipy \ref{scipy}, al cual se le pasa las posibles etiquetas y la predicción para cada una de estas etiquetas de cada muestra. Después, se generan 10 particiones de tamaño él de la base de datos de test donde en cada partición un de las partes es seleccionada para test sin ser repetidas y las restantes para train, donde el contenido son los indices de correspondientes a las muestras, haciendo uso de KFold \ref{KFold} y se realiza una copia del array de predicción generado al principio de este paso. Seguidamente para cada una de las partes generadas, se procede a añadir a los datos de la base de datos de train la parte seleccionada para cada caso de train de la base de datos de test, utilizando la función concatenate \ref{concatenate} del paguete numpy \ref{numpy} , seleccionando los datos correspondientes a la parte de test de esta partición y inicializando un array llamado pr\_vals con un array vacio donde se irán añadiendo la predicción de cada partición. A continuación, se construyen 20 modelo para cada conjunto de datos, para lo cual lo primero que se realiza, es obtener a partir de las distribuciones de probabilidad generadas anteriormente un valor aleatorio para la etiqueta de la clase para las muestras de la base de datos de test, debido a que es necesaria esa etiqueta, ya que se ha aplicado la base de datos de train con muestras de la base de datos de test y les faltan a estas ultimas ese valor, y guardan en una variable llamada y\_train en un array de numpy \ref{numpy} los valores de la etiqueta clase de la base de datos train y los valores obtenidos para las muestras seleccionadas de la base de datos de test. El siguiente paso consiste en generar dos arrays de indices uno de el tamaño de los datos ampliados y otro con el tamaño de los datos seleccionados de la base de datos de test. A continuación, se procede a realizar una permutación del array de indices completo, para posteriormente ampliarlo con siete veces del tamaño de los datos seleccionados de la base de datos de test, añadiendo de manera aleatoria muestras de este subconjunto. Se continua, generando las DMatrix que se utilizaran en el modelo, siendo una con los datos completos para train con su correspondiente valores para la etiqueta y la correspondientes a los valores que se han dejado para el test. También se establecen los parámetros para correspondiente modelo utilizando un diccionario, siendo para  este caso num\_round con el que se establece que el número de rondas, seed con el que se establece la semilla para la generación de números aleatorios, siendo en este caso $123+13141*i$ donde i es el indice del modelo, max\_depth por el cual se establece que la profundidad de los arboles que construirá el modelo sea como máximo de 3, gamma con el que se establece el nivel de perdida mínima necesaria para que se sigue expandiendo un nodo según una función de perdida, eta por el cual se establece un peso en tanto por 1, en este caso 0,22, con el que se ponderan los pesos que va generan durante la construcción del modelo para establecer la importancia del modelo construido según su acierto a la hora de predecir las muestras proporcionadas al modelo, con el valor que se le ha proporcionado hace que el comportamiento del modelo sea conservador teniendo mas importancia las características ya aprendidas que las nuevas características, silent con el que se establece que no se muestre información durante la generación del modelo, objective con multi:softprob por el cual se establece que el resultado sera una matriz donde para cada elemento se mostrara la probabilidad de cada una de las posibles etiquetas de clase, num\_class con un 9 con el que establece que el número de etiquetas para clasificar la clase ese de 9 según se establece en el reto, subsample por el cual se establece el porcentaje en tanto por 1 de muestras que son seleccionadas para la construcción del modelo, colsample\_bytree con el que se establece el porcentaje en tanto por 1, el número de características que van a ser seleccionadas para construir el árbol de decisión en cada iteracción y nthread por el cual se establece que se generen 16 hilos para construcción de los modelos. También, se crea una array vació que se llama watchlist que establece que no se revisa los resultados del modelo con un conjunto de muestras que no es utilizado en el entrenamiento. A continuación, se entrena el modelo XGBoost \ref{xgboost} con los datos y parámetros generados y después se realiza la predicción con los datos de test seleccionados, para a continuación añadirlos al array pr\_vals. Una vez terminados los 20 modelos se obtiene la media de los resultados almacenados en el array pr\_vals y se concluye por guardar este resultados en el la copia del array de predicciones. Para concluir, una vez completado todo este proceso para todas las particiones se abre el fichero 'sampleSubmission.csv'\ con la función read\_csv \ref{read_csv} del paquete pandas \ref{pandas}, se le asignan los resultados guardado en la copia del array de predicciones y se guardan los resultados en un fichero llamado 'release1.csv'. Habiendo terminado así la construcción de todos los modelos necesarios para esta solución.\\

En este punto, solo queda abrir dos los archivos de predicción y el fichero 'sampleSubmission.csv' con la función read\_csv \ref{read_csv} del paquete pandas \ref{pandas} para guardar en el dataframe de este ultimo fichero una media pondera de los dos ficheros de predicción obtenidos, siendo un 95 \% el ultimo modelo y el resto el otro modelo utilizado para la predicción y se concluye guardando los resultados en un fichero llamado 'release2.csv', el cual es el que obtuvo la calificación indica al principio.\\


\chapter{METODOLOGÍA Y DESARROLLO}

Llegados a este punto, procederé a explicar el procedimiento llevado acabo para la obtención de la solución del reto propuesto.\\

Conforme se estableció en la planificación del anteproyecto, el primer paso era la familiarización con la seguridad informática y concretamente con los conceptos necesarios para el aborde de este reto. Por esto, además de realizar una búsqueda de soluciones propuestas por otros equipos, las cuales se ha explicado en el apartado de antecedentes y estado de la cuestión, también se busco artículos en los que se mostrará métodos de análisis que pudieran ser aplicados a este reto. De la cual, se obtuvo información de las familias de malware utilizados en este reto y sus correspondientes características identificativas, la cuales ayudaran a elegir cuales serán las características mas adecuadas a utilizar para el reto, siendo los tipo de malware utilizados los siguientes, según la página de Glosarios de Microsoft Windows Defender Security Intelligence \cite{glossary_windows_defender_security_intelligence}:\\


\begin{enumerate}
	\item Worm o Gusano\label{worm} es un tipo de malware que se propaga a otros ordenadores. Este puede usar para propagarse uno o más de los siguientes métodos: Archivo adjunto o link en un email, envío a través de un sistema de mensajería instantánea como puede ser Skype, en los ficheros descargado o subidos en redes de intercambio de ficheros p2p, enviando un mensaje a todos tus contactos de una red social, en el auto-arranque de un disco extraíble o explotando una vulnerabilidad de un software. 
	\item Adware\label{adware} es un tipo de malware que muestra publicidad adicional que no puedes controla muestas estas usando el ordenador.
	\item Trojan\label{trojan} es un tipo de malware que a diferencia del worm \ref{worm} no se propaga por si solo. Esto hace que tenga que aparentar ser un software inofensivo y que la victima lo descargue e instale para de ese modo infectar su ordenador. Un vez, el sistema esta comprometido puede robar tu información personal, descargar otro malware y pudiendo llegar a permitir al atacante tener acceso y/o control sobre tu ordenador.
	\item Backdoor\label{backdoor} este tipo de malware es un clase de trojan \ref{trojan} que permite al atacante tener acceso y control a tu ordenador.
	\item TrojanDownloader\label{trojandownloader}, al igual que en el caso del backdoor \ref{backdoor}, este tipo de malware es una clase de  trojan \ref{trojan}, la cual instala otros ficheros maliciosos, incluido malware, en tu ordenador. El puede descargar el fichero desde un ordenador remoto o instalarlo directamente desde un copia incluida dentro de él.
	\item Any kind of obfuscated malware\label{obfuscated} no se trata de un tipo de malware, sino que son técnicas usadas para ocultar o hacer parecer limpio un malware, aunque en la página de glosarios de términos \cite{glossary_windows_defender_security_intelligence} que estamos utilizando si lo identifican como un tipo de malware. Lo que realizar es ocultar su código para hace mas difícil que los programas de seguridad lo detecten o eliminen.
\end{enumerate}

De estos tipos de malware, se han seleccionado las familias que utilizaremos en el reto, siendo estas:\\

En primer lugar la familia Ramnit \label{Ramnit} que según la información del reto es de tipo Worm \ref{worm} aunque en la propia página de Microsoft Defender Security \cite{win32/ramnit_windows_defender_security_intelligence} no se llega a clasificar, aunque se puede deducir ya que si se menciona que el vector de entrada más habitual es mediante la utilización de un disco extraíble, generalmente pendrive, ya infectado. Este malware roba información sensible del usuario, como pueden ser usuario y contraseña de acceso a su banco. Además, puede proporcionar acceso y control de nuestro ordenador al atacante.\\

En segundo lugar nos encontramos la familia Lollipop \label{Lollipop} que según la información del reto y la propia página de Microsoft Windows Defender Security \cite{adware:win32/lollipop_windows_defender_security_intelligence} es de tipo Adware \ref{adware}. Esta familia de malware lo que realiza es mostrar publicidad emergente mientras se esta utilizando el navegador y redirige los resultados de tu motor de búsqueda. Debido a que redirige los resultados del motor de búsqueda, le permite mostrar la publicidad  basándose en las palabras claves utilizadas en esto además de su ubicación geográfica.\\

En tercer lugar es el turno de Kelihos\_ver3 \label{Kelihos_ver3} que según la información del reto es de tipo Backdoor \ref{backdoor} y según la propia página de Microsoft Windows Defender Secutiry \cite{backdoor_win32/kelihos.f_windows_defender_security_intelligence} lo clasifican como un trojan \ref{trojan} que puede dar acceso y control  de nuestro ordenador al atacante, que conforme se puede observar es la definición de backdoor \ref{backdoor}. Es la tercera versión de este malware y se propaga mediante el envío de correos electrónicos no deseados que contienen enlaces a otros malware. Además, puede comunicarse con otros ordenadores para intercambiar información sobre el envío de correos electrónicos no deseados, robar tu información sensible o descargar y ejecutar ficheros maliciosos.\\

El siguiente es Vundo \label{Vundo} que según la información del reto es de tipo Trojan \ref{trojan} aunque en la propia página de Microsoft Windows Defender Security \cite{win32/vundo_windows_defender_security_intelligence} lo clasifica como una familia de malware formada por múltiples familias, el cual muestra ventanas emergentes con publicidad fuera de contexto. Ademas, algunas de sus variantes puede descarga y ejecutar otros ficheros, incluyendo otros tipos de malware. El vector de entrada mas habitual es mediante un complemento para el navegador. También, utiliza técnicas avanzadas para defenderse y mantenerse a salvo de las detecciones y ocultarse cuando se eliminan.\\

A continuación, nos encontramos con Simda \label{Simda} que según la información del reto y la propia página de Microsoft Defender Security \cite{win32/simda_windows_defender_security_intelligence} es de tipo Backdoor \ref{backdoor}. Lo que realizar es robar la contraseñas del usuario del sistema infectado.\\

El siguiente en la lista es Tracur \label{Tracur} que según la información del reto es de tipo TrojanDownloader \ref{trojandownloader} pero en la propia de página de Microsoft Defender Security \cite{win32/tracur_windows_defender_security_intelligence} lo clasifica como una familia de Trojans \ref{trojan} que puede redirigir tus buscadores web. Realizan esto para ganar dinero con la publicidad fraudulenta. Este malware también puede descargar y ejecutar otros ficheros, esto incluye otros posibles malware, y de esta manera proporcionar al atacante el control de tu ordenador. Los vectores de entra mas habituales por este malware son que sea instalado por otro malware, cuando haces click en enlaces sospechosos o mediante un archivo adjunto en un email.\\

A continuación, es el turno de Kelihos\_ver1 \label{Kelihos_ver1} este tipo de virus es la primera versión del ya mencionado Kelihos\_ver3 \ref{Kelihos_ver3}, el cual en le reto es clasificado como de tipo Backdoor, mientras en la propia pagina de Microsoft Windows Defender Security \cite{win32/kelihos_windows_defender_security_intelligence} lo clasifican como un trojan \ref{trojan}, igual que ocurría con Kelihos\_ver3.\\

Siguiendo con la lista nos encontramos con Obfuscator.ACY \label{Obfuscator.ACY} que según la información del reto y la propia página de Microsoft Windows Defender Security \cite{virTool:win32/obfuscator.acy_windows_defender_security_intelligence} se trata de una técnica mediante la cual se intenta ocultar \ref{obfuscated} el malware y así evitar que los sistemas de seguridad la detecten.\\

Y por ultimo, nos encontramos con Gatak \label{Gatak} que según la información del reto es de tipo Backdoor \ref{backdoor} y la propia página de Microsoft Windwows Defender Security \cite{win32/gotak_windows_defender_security_intelligence} es de tipo Trojan \ref{trojan}. Según la página mencionada, esta familia de malware recoge información del equipo infectado y enviarla posteriormente al atacante y el punto de entrada suele ser a través de un generador de llaves para una aplicación o mediante una aparente actualización legitima de una aplicación. Una de las acciones mas significativas que realiza que ademaspermite identificar que estamos infectados con este malware es modificar el registro de Microsoft Windows para que la aplicación de mensajería y videollamada  Google Talk se arranque de manera automática y Skype se inicie de manera automática de manera minimizada y sin mostrar la pantalla durante esta arrancando. Esto es llamativo porque estos aplicaciones pueden ser utilizadas para exfiltrar la información recabada del usuario.\\

Según se nos explica en el siguiente articulo \cite{DBLP:journals/corr/abs-1802-10135}, para este reto se nos proporcionan dos base de datos, train y test, las cuales contienen muestras de malware de los clases mencionadas anteriormente. Estás están formadas por dos tipos de fichero, por un lado un fichero en binario, al cual se le ha eliminado la cabecera para hacerlo inofensivo, representado en formato hexadecimal y otro que contiene metadatos obtenidos mediante el desensamblado de las muestras con la utilidad IDA dissambler tool. Donde la base de datos de train está compuesta por 10868 muestras de malware las cueles se reparten por las clases de malware según muestra en la tabla \ref{malwareClassDatasetTrain}, en cuanto a la base de datos de test como esta formada por 10873 muestras de malware, las cuales ahí que clasificar en la clase que le corresponde.

\begin{table}[hbtp]
\centering
\begin{tabular}{p{0.1cm}p{1cm}p{0.1cm}p{3cm}cc}
\hline
& Id &\multicolumn{2}{c}{Clase} & Muestras & Tipo \\ \hline
& 1  && Rammit & 1541 & Worm \\
& 2  && Lollipop & 2478 & Adware \\
& 3  && Kelihos\_ver3 & 2942 & Backdoor \\
& 4  && Vundo & 475 & Trojan \\
& 5  && Simda & 42 & Backdoor \\
& 6  && Tracur & 751 & TrojanDownloader \\
& 7  && Kelihos\_ver1 & 398 & Backdoor \\
& 8  && Obfuscator.ACY & 1228 & Any kind of obfuscated malware \\
& 9  && Gatak & 1013 & Backdoor \\ \hline
\end{tabular}
\caption{Clase de malware en la base de datos train.}
\label{malwareClassDatasetTrain}
\end{table}

Además, se consiguio el artículo `A scalable multi-level feature extraction technique to detect malicious executables` de Mohammad M. Masud, Latifur Khan y Bhavani Thuraisingham de 2007 \cite{masud2008scalable}, en el cual se explica la investigación que han realizado con respecto la detección de malware. Esta investigación demuestra que una extracción de características mixta mejora los resultados de los modelos. Esta perspectiva híbrida supone extraer y manejar características de los ficheros binarios en función del número de ngrams \ref{ngram}, instrucciones en ensamblador, llamadas a funciones tanto del sistema como propias, etc. Esto supone incrementar y mucho el número de características, haciendo necesario realizar un proceso de selección de las mas significativas para que el modelo a construir sea abordable. Resulta interesante que se trata de la misma perspectiva utiliza en la solución 2 \ref{solution2}. Para mi solución, he decidido utilizarla también.\\

Llegado a este punto, comenzaré a explicar el proceso llevado para la construcción de mi solución.\\

La primera decisión, después de tener claro que se va a utilizar un sistema de características híbridas, es el número de ngrams \ref{ngram} que se se van a utilizar. Una vez estudiado tanto el artículo anterior como la solución 2 \ref{solution2}, tomé la decisión de hacer un esfuerzo para utilizar en mi solución los mismos 4 ngrams que se utilizaba en la solución 2 \ref{solution2}. Esta decisión es más importante de lo que parece, ya que va repercutir en todas las decisión posteriores, debido a los recursos de los que voy a hacer uso son solamente mi portátil personal (Toshiba L850-150) y mi disco duro externo, siendo las características las siguientes:\\

\begin{itemize}[nolistsep]
	\item[•] Procesador Intel Core i7-3610QM a 2,30 GHz.
	\item[•] 8 Gb RAM DDR3.
	\item[•] Tarjeta Gráfica Radeon 7670M.
	\item[•] Disco Duro SSD Samsung EVO 850 de 250 Gb.
	\item[•] Disco Duro externo Seagate Expansion de 4 Tb.\\
\end{itemize}

Estos recursos implican no poder tener todas las características extraídas en memoria ni tan siquiera de una sola muestra. Esto hace que sea necesario utilizar alguna tecnología que permita almacenar de manera persistente esta información. Para solucionar este problema, la idea fue utilizar una base de datos, concretamente MongoDB \ref{mongodb}. El motivo por el que me decante por está me decidí por utilizar esta base de datos fue que se trataba de una base de datos NoSQL \ref{nosql} con lo facilidad que esto implica ya que te permite almacenar en los atributos estructuras como pueden ser arrays y objetos BSON \ref{bson}, además tiene una buenas compatibilidad con el lenguaje de programación elegido Python 3 y es una opción muy estable, consolidada, eficaz y open source.\\

Otra cuestión a tratar antes de entrar a explicar la configuración de la base de datos MongoDB, es explicar el motivo por el cual se ha tomado la decisión de utilizar docker y la configuración que se ha llevado a acabo. Para empezar el motivo, ha sido para fomentar la portabilidad de la solución y facilitar su ejecución y prueba. También, se ha buscado de alguna manera controlar los recursos que dispongo y aislar la ejecución del sistema, ya que se manejan muestras de malware y aunque se hayan tomado medidas para convertirlos en inofensivos es mejor aislarlos. Por estos motivos y por tratarse de una sistema de virtualización ligero se ha optado por la utilización de Docker \ref{docker}.\\

En primer lugar, se tiene que instalar Docker y Docker Compose conforme se puede en los siguientes enlaces \cite{docker_install} y \cite{docker_compose_install} respectivamente. En este punto, comenzaremos con la configuración para lo cual se utilizar un fichero de tipo yaml donde se establecerán la configuración que posteriormente utilizará Docker Compose. En este fichero, se crearán dos servicios, db y tfg, y una red, cuyo nombre se tfg, para posibilitar la comunicación entre los servicios.\\

La red se llama tfg y se le ha indicado que el driver es bridge, mediante el cual se establece que los contenedores se mantienen aislados y utilizan esta red virtual para comunicarse entre ellos. También se le configura que el sistema administrador de direcciones IP utiliza como driver el que tiene establecido por defecto y que el segmento de red que utilizarán los contenedores es 172.16.0.0/24.\\

El servicio db se establece la configuración de la base de datos de MongoDB \ref{mongodb}, para lo cual se establece que la imagen que se va a utilizar el la última disponible de mongo mediante el parámetro image:mongo:3.6.5. A continuación los parámetros hostname y container\_name establece el nombre de la maquina y el nombre del contenedor que se creará respectivamente, en mi caso son mongo y mongo\_tfg. El siguiente parámetro es command mediante el cual se establece el comando que se ejecutará al lanzar el servicio, en mi caso los que se ejecutará el servidor de MongoDB \ref{mongodb} con su correspondiente configuración. La configuración que se ha decido establece para el servidor de base de datos MongoDB \ref{mongodb} es utilicé como motor de base de datos wiredTiger con el parámetro storageEngine, que cree un directorio por cada base de datos con el parámetro directoryperdb, que guarde en un directorio los indices y en otro las colecciones con el parámetro wiredTigerDirectoryForIndexes, que comprima las colecciones con el algoritmo que se establece con el parámetro wiredTigerCollectionBlockCompressor, en mi caso se ha establecido zlib, ya que es el mas eficaz a la hora de reducir espacio, que comprima también los indices con el parámetro wiredTigerIndexPrefixCompression y por ultimo se le establece la cantidad de RAM que utilizará con le parámetro wiredTigerCacheSizeGB, en caso le he establecido 4 Gb. El siguiente parámetro que se establece en el servicio es volume mediante el cual se establece un almacenamiento permanente al contenedor y en este caso también se le establece la dirección dentro del propio ordenador donde se almacenará, en este caso se ha optado por almacenarlo en el el directorio home de mi ordenador en la carpeta TFGDataBase que esta ubicada en el disco solido.\\

Por ultimo, ya solo nos queda establecer la configuración de red que tendrá el contenedor para la red que se ha establecido para los contenedores, siendo la configuración una dirección IP dentro del rango establecido en la red tfg con el parámetro ipv4\_address y alias con el parámetro alieses, siendo los valores para estos 172.16.0.2 y mongo respectivamente.\\

Por ultimo, el servicio tfg \label{servicio_tfg} al igual que en el comando db se le ha establecido el nombre a la maquina, el nombre al contenedor, los volúmenes que establecen la ubicación del almacenamiento permanente y la configuración de red del contenedor. Además de esta configuración, se utiliza el parámetro depends\_on para indicar que este servicio necesita de manera obligatoria el servicio db, con lo cual tiene que ejecutarse del servicio db antes de que comience la ejecución de éste, y el parámetro enviroment donde se establecen una listas de variables de entorno con las que se ajustará la ejecución de la solución. Estas variables de entorno son SAMPLE con la cual se indica que seleccione una muestra de de los fichero de train, PERCENTAGE\_SAMPLE con el que se le indica el porcentaje concreto de la muestra, solo es utilizado conjuntamente con la variable SAMPLE, PROJECT con la cual se le asigna un nombre al proyecto, IP\_SERVER\_MONGO con la que se le dice la dirección IP que se le ha asignado al servico db, DB\_NAME con el cual se le indica el nombre de la base datos que va a manejar el proyecto, ORDER\_EXTRACT\_FEUTURES\_ASM con la que se establece el número de ngrams \ref{ngram} que se van a utilizar en la solución, DIR\_DATA\_RAW con la cual se le indica la dirección donde se encuentran los datos que maneja la solución, SEED con el que se le establece una semilla, THREAD con el cual se le indica el número de hilos que se utilizará en los procesos de paralización, PERFORM\_METHOD con el que se le indica que parte de la solución se ejecutara y NUM\_FEATURES con la cual se se le indica el número de características que se utilizara en el modelo que se construya. También se le establece que el nombre del contenedor sea code\_tfg, el nombre de la maquina sea code y la imagén que utilice para crear el contenedor sea tfg, aunque en este caso esta no es una imagén oficial, como en el caso de MongoDB \ref{mongodb}, esto implica que se tiene que  construir está, por lo cual también se tiene el parámetro build . Este parámetro hace que tengamos que crear un fichero llamado Dockerfile donde estableceremos la configuración especifica para generar esta imagen. En este fichero se le configura el parámetro FROM con el cual se le indica que utilice como base la imagen oficial python:3.6.5, el parámetro MAINTEINER mediante el cual se le establece el autor de la imagen, el parámetro LABEL con el que se le establece una etiqueta y se le establece un nombre al proyecto, siendo este detectorMalware, el parámetro ADD con el que se copia los ficheros de la maquina huésped a la maquina virtual dentro del contenedor, el parámetro RUN con el que se ejecutan comandos dentro de la maquina del contenedor, en mi caso va al directorio que se ha copiado y instala pip3 install -r requirements.txt los paquetes necesarios para ese proyecto, el comando WORKDIR con el que se establece como directorio inicial el directorio donde hemos copiado los ficheros y por  ultimo el parámetro CMD con el cual se establece el comando que va a ejecutar cuando se inicie el contenedor, en mi caso python3 detectorMalware.py. Con esto se termina la configuración de Docker \ref{docker}.\\

El directorio, que se copia en el servicio tfg \ref{servicio_tfg} que contiene la implementación de mi solución, se estructura de la siguiente manera. En este directorio tenemos el fichero requirements.txt en el cual establecemos los paquetes de python necesarios para  ejecutar mi solución, el fichero \_\_init\_\_.py en el que se establece metainformación del proyecto, como puede ser nombre del paquete, versión y autor, el fichero dectorMalware.py el cual es el encargado de ejecutar y controlar la ejecución de mi solución y por ultimo el directorio src el cual se subdivide en 4 directorios, siendo los siguientes extract\_features el cual contiene los ficheros necesarios para llevar a cabo la fase de extracción de características, misc en el que se almacena los ficheros varios que se utilizarán en la solución, por ejemplo sample, split entre otros, model el cual contiene los ficheros que manejan desde el preprocesado de los datos hasta la predicción del modelo posando por la construcción del mismo y mongo donde se guarda el fichero que establece la conexión con la base de datos MongoDB \ref{mongodb}.\\

Comenzaré la explicación de mi solución siguiente el proceso de ejecución del fichero detectorMalware.py, siendo el primer paso recuperar los valores de la variables de entorno establecidos en el fichero docker-compose.yml que se comentaron anteriormente. El siguiente paso, es comprobar si se desea ejecutar la solución sobre una muestra, esto se realiza así debido a que el volumen de información a manejar es excesivamente grande y durante el proceso de implementación sería muy tedioso tener que esperar el tiempo que supondría ejecutarlo sobre el conjunto completo de datos, por ese motivo se ha optado por esta opción para reducir el tiempo que supondría. Para seleccionar la muestra se la creado un fichero sample.py dentro del directorio misc donde se ha implementado la función selectSample, el cual selecciona una muestra estratificada del fichero trainLabels.csv, para lo cual hace uso de pandas \ref{pandas} creando un DataFrame \ref{dataframe} y seleccionando de manera aleatoria una muestra de los valores de la clase 1 y repitiendo es proceso para el reto de valores de la clase y concatenandolos en el dataframe \ref{dataframe} del primer valor con el comando de pandas \ref{pandas} concat \ref{concat} para terminar guardando el dataframe \ref{dataframe} en un nuevo fichero csv y terminado por devolver la dirección con esta almacenado el mismo.\\

A continuación, se abre el origen ya sea el proporcionado por en el reto o en el generado durante el proceso de muestra, para lo cual se hace uso de la función open\_sources la cual abre el el fichero de donde se van a leer los datos que se pasa a la función por parámetro y leyendo la información del fichero. Se continua separando esta información por lineas y desechando la primera porque son las cabeceras del fichero y la ultima porque en el fichero ahí una linea la final del documento sin información. Después, se obtiene el número de lineas que tiene el documento y se parte cada una de las lineas con una función aplicándole una función que se le pasa por parámetro a la tupla obtenida de aplicar la función enumerate a los datos. Por ultimo, se cierra el fichero y se devuelve la longitud y los datos ya particionados. Las funciones para realizar el particionado de los ficheros origen se encuentran en fichero split en misc. En este fichero tenemos la función para particionar los datos de origen de train, de test y una función genérica común para ambas funciones. La función split\_Source lo que hace generar un array en el que la primera posición es el indice que tiene ese elemento en el fichero origen seguido de todos los elementos que posee el elemento después de dividirlos por la coma y quitarle las comillas dobles. En el caso de split\_Source\_Test se devuelve una tupla de los dos primeros elementos del array que general la función general. En cambio, en split\_Source\_Train se hace igual pero el tercer elemento del array se transforma de string a un entero.\\

El siguiente paso, es comprobar el valor que tiene la variable de entorno PERFORM\_METHOD. Los posibles valores que se le puede asignar son EXTRACT\_TRAIN, DISCRETE, SELECT, MODEL, EXTRACT\_TEST, MODEL\_PREDICT, PREDIT, EXTRACT\_ALL Y ALL que establece si se extraen las características de la base de datos de train, si se discretizan las características de la base de datos de train, si se seleccionan las características que mayor información proporcionan de la base de datos de train, si se crea el modelo con las características seleccionadas de la base de datos de train, si se extraen las características de la base de datos de test, si se crea el modelo y se predice el resultado de la base de datos de test, si se predice el resultado de la base de datos de test, si se extrae las características de las dos bases de datos o todo lo anterior respectivamente. Esto se ha realizado así para durante el proceso de implementación poder excluir los pasos ya concluidos y ahorrar todo el tiempo que fuese posible.\\

El primero de estos pasos que ahí que ejecutar, por orden es EXTRACT el cual, lo primero que realizar establecer la conexión con la base de datos para los cual se usa la clase creada en el fichero database.py dentro de la carpeta mongo. 
En este fichero, contiene una clase llamada DataBaseMongo la cual se ha creado para manejar y controlar la conexión con la base de datos, para lo cual cuando se crea una instancia de la misma ahí que indicarle tanto la dirección del servidor de base de datos MongoDB \ref{mongodb} y el nombre de la base de datos a la que nos queremos conectar. Esta clase también posee un método para seleccionar una colección pasándole como parámetro el nombre de la misma. Esto se ha realizado así para garantizar de que no se le pase ningún parámetro erróneo y para manejar los posible excepciones tanto de la conexión con el servidor como la selección de la colección.\\

Una vez, establecida la conexión con la base de datos se procede a borrar todas las colecciones que tiene establecidas, para lo cual selecciona cada colección y la elimina con el método drop proporcionado para la API de MongoDB \ref{mongodb} para python.\\

A continuación, se procede a la extracción de las características de la base de datos train, proceso que se realizada de manera paralela utilizando el número de hilos que se ha establecido en la configuración, y realizando así la extracción de las características de manera simultanea. Este proceso de extracción consisten en llamar en la función extract\_features dentro del fichero extract\_features.py, al cual ahí que pasarle por parámetro la ip del servidor de base de datos MongoDB \ref{mongodb}, el nombre de la base de datos a la que nos conectaremos, la ubicación dentro del contenedor donde se puede acceder a los datos proporcionado, el prefijo y el nombre de las colecciones que se utilizarán, el número de ngrams \ref{ngram} a extraer y por ultimo del fichero del que se extraerán las características, siendo todos los parámetros fijos salvo este ultimo que cada hilo tratará uno de estos ficheros. Los datos proporcionados se la pasan al sistema en un fichero comprimido en zip de donde se seleccionan el elemento a procesar. Esto se ha realizado de esta manera por ahora espacio en el disco duro y ha sido sacada la idea de la solución 1 \ref{solution1}, ya que de esta manera el peso de las dos bases de datos es de algo mas de 56 Gb mientras si se trata de manera descomprimida ocuparía mas de 400 Gb complicando donde almacenara tanta información, aunque como no se podía manera los archivos comprimidos que se suministran a tenido que descomprimirse el fichero 7z y posteriormente comprimirlo de nuevo en formato zip, el cual si es posible manejarlo con python. Este proceso se realiza por separado tanto a la base de datos de train como para test. Llegados a este punto, el primer pasa es establecer la conexión con la base de datos y seleccionar la colección train\_total donde se inserta un registro con el id, nombre y etiqueta del elemento que se va a procesar en dicha colección. El siguiente paso consiste en realizar las modalidades de extracción que se van realizar, extracción de de binario y de ensamblador, para lo cual se se utilizar la función extract\_Features\_Byte dentro del fichero extract\_features\_bytes.py y la función extract\_Features\_Asm dentro del fichero extract\_features\_asm.py respectivamente. Empezaré explicando la función extract\_Features\_Byte, la cual se ha adaptado la implementación de la solución 2 \ref{solution2} que se esta parte de la solución a mi solución, siendo lo primero se conecta con la base de datos de MongoDB \ref{mongodb} y seleccionar tanto la colección total como la features donde se almacenará la información extraída. Después, se abre de dentro del fichero comprimido en zip para esa base de datos el fichero establecido en el origen y se acumulan todos los ngrams \ref{ngram} en un array para posteriormente recorrer este array en bloque establecidos por el parámetro order que establece el número de ngrams \ref{ngram} que se extraerán llevando el conteo en un diccionario. Se continua recorriendo este diccionario para obtener el número total de ngrams \ref{ngram} y actualizar el valor del registro insertando anteriormente en la colección total con el número total de estos del fichero, esto es realizado así para tener los totales y realizar una normalización de los conteos respecto del fichero. Para concluir, insertando todos los elementos del diccionario en la colección features donde se  almacena para cada bloque de ngrams \ref{ngram} del fichero en id del fichero, el nombre del fichero, la característica que esta formada por "b\_" \ seguido del bloque de ngrams \ref{ngram} que corresponde, el número de veces que aparece en el fichero tanto normalizado respecto del fichero como sin normalizar y el tipo de característica de que se trata, siendo en este caso bytes.\\

En cambio en la función extract\_Features\_Asm, donde se realiza la extracción de la información de los ficheros en ensamblador, el proceso que realiza empieza igual que en el caso de bytes donde se establece la conexión con la base de datos, se seleccionan las colecciones que se utilizarán que son total y features y se obtiene de dentro del fichero comprimido en zip el fichero establecido como origen para extraer los datos, aunque en este punto se le establece que se encuentra codificado en iso8859 debido a que en el paso anterior no era necesario al no manejar caracteres especiales. A continuación, se parte el fichero en lineas haciendo uso del paquete re \ref{re} con la función findall y la opción multiline, esto se realiza así para reconocer varios tipos de salto de linea y evitar así de esta manera posibles problema de codificación. En este punto, se elimina de la linea los posibles saltos de carro, los comentarios, los espacios en blanco al inicio y los tabuladores, para continuación partiendo la linea una vez por el separador ":" \ y obteniendo así la sección y la linea que queda pendiente de tratar. El siguiente paso, es partir la linea una vez teniendo en este caso como separador " " obteniendo en este paso la dirección y almacenando en un diccionario la sección extraída anteriormente un conjunto donde se guardaran las direcciones encontradas, esto es así para solo contemplar una vez cada una de las direcciones. A continuación, si la linea pendiente de procesar no esta compuesta solo de espacios en blanco, entonces se divide la parte binaria para lo cual se utiliza la partition\_bytes, debido a la complejidad que tienen las comprobaciones necesarias se creo esta función para no complicar mas la función principal. En esta función, se comprueban todos los posibles  patrones reconocibles, siendo que sean un conjunto de ngrams \ref{ngram} que terminan con al menos dos espacios en blanco, que sea un conjunto ngrams \ref{ngram} y termine con el signo +, que sea un conjunto de ngrams \ref{ngram} seguido te únicamente espacios en blanco o por ultimo que contenga al menos un espacio en blanco. En todos los casos se parte según el patrón que se ha obtenido, salvo en el ultimo de los casos que  se parte por el espacio en blanco y se comprueba si cada parte corresponde con el patrón de un ngram \ref{ngram} y con una variable se controla la posición que tenemos en el array hasta ese momento y terminando por formar un array donde el primer valor son los elementos hasta la ultima posición que coincide con el patrón de los ngrams \ref{ngram} y transformando el array en string y como segundo valor el resto  de los elementos también transformándolos de array a string. Para esta transformación se utiliza la función arrayToString dentro del fichero de string dentro de misc. Esta función lo que realiza es coger el primer elemento del array y concatenarle el resto de elementos con el un espacio en blanco entre medias. Un vez terminada obtenido la parte binaria, solo nos queda partir la linea pendiente de procesar teniendo como separador al menos un espacio en blanco. De esta ultima partición obtenemos como primer elemento la instrucción y el resto de elemento son argumentos de esta. A continuación, se parte la instrucción si contiene algún guión bajo que no este al principio o al final y se selecciona como instrucción el primer de los elementos y concatenando el resto una vez transformado en string con la función arrayToString con la linea sin procesar con un espacio en blanco entre medias. Se continua limpiando de la instrucción de los posible guiones bajos, de los dos punto, de los iguales y de las comas los espacios en blanco que tuvieran alrededor. Ahora ya solo queda limpiar de espacio en blanco al inicio y al final la linea pendiente de procesar y partirla teniendo como separador al menos un espacio en blanco y obteniendo un array de argumentos, en el caso de no tener espacio en blanco se tiene un array con un solo elemento que es al linea pendiente de procesar. Por ultimo, en el caso de  que la instrucción sea `call`\ se le añade a la instrucción el primero de los elementos de los argumentos y guardando la instrucción en un diccionario. Para terminar los dos diccionarios para obtener el número total de instrucciones y secciones y actualizar el valor del registro insertando anteriormente en la colección total con los totales del fichero, esto es realizado así para tener los totales y realizar una normalización de los conteos respecto del fichero. Para concluir, insertando todos los elementos de los diccionarios en la colección features donde se almacena para cada bloque de instrucción o sección del fichero en id del fichero, el nombre del fichero, la característica que esta formada por `a\_` \ o `s\_` \ seguido del bloque de de instrucción o sección respectivamente, el número de veces que aparece en el fichero tanto normalizado respecto del fichero como sin normalizar y el tipo de característica de que se trata, siendo en este caso asm o section.\\

Un vez se termina de la extracción de todos las características de la base de datos de train se crea un indice de las características feature y index\_file ambos en orden ascendente. Terminado así el proceso de extracción de las características del a base de datos de train.\\

En este punto, es cuando me doy cuenta que todas las variables que voy a utilizar tienen valor continua complicando de ese modo el manejo de los mismo. Se buscan posibles soluciones, como son calculo de la entropía de Shannon a través de una función de probabilidad, discretización, etc., terminando por seleccionar la discretización de las variables como mejor opción. Para ser concreto el sistema de distretización que se utilizará es el utilizado por el árbol de decisión C45, el cual se estudio durante la especialización en la asignatura Sistemas Basados en el Conocimiento. Es un sistema muy sencillo y esta comprobado que obtiene buenos resultados. Mi implementación se llevará a acabo en la función calculateDiscreeting dentro del fichero selection\_filter.py apoyándome en todo momento en la base de datos MongoDB \ref{mongodb}.\\

En esta función, lo primero que realiza conectarse con la base de datos, seleccionar las colección features y discreeting, y borrar la colección discreeting todas las colecciones con su prefijo correspondiente. A continuación, se crea un array de numpy \ref{numpy} de 1 por en número de elementos de la base de datos a tratar con valores de tipo entero del 0 hasta el tamaño de la base de datos menos 1 y una variable llamada processed donde se almacenan el número de elementos procesados hasta el momento, como es lógico el valor inicial de esta variable es de 0. El siguiente paso es un bucle infinito que terminará cuando se termine el cursor sin que se produzca el error CursorNotFound, el cual ocurre cuando el cursor supera el tiempo máximo de espera establecido. El cursor al que se hacia mención anteriormente es el obtenido de un array con los indices y su correspondiente etiqueta insertados por orden ascendente del número de apariciones normalizadas y etiqueta, ademas de ordenador por característica, y saltándose los elementos ya procesados hasta el momento. A continuación, se recorren todos los elementos del cursor dentro de try except para manejar en el caso de que de error CursorNotFound mencionado anteriormente. Para cada uno de estos elementos, se incrementa en 1 la variable processed, se comprueba si la característica aparece al menos en dos muestras sino se salta a la siguiente característica. A continuación, se crea un array para las partes discretizadas y otro para la partición actual y una variable llamada num\_part donde se almacena el indice de la partición actual empezando desde 1. Además, se crea otros dos array uno con todos los posibles indice y otro de numpy \ref{numpy} donde de almacenará en que partición se encuentra cada indice, esto se hace así para durante el proceso de discretización calcular también la entropía de la característica que se utilizará posteriormente. En este punto, en cuando comenzamos con el proceso de discretización, teniendo como primer paso seleccionando la información del primer elemento, indice y etiqueta, y almacenándolos en index\_element\_prior y label\_element\_prior respectivamente. A continuación, se elimina el indice del array de indices y se añade al array de la partición actual. Después, se recuperan los datos indice y etiqueta del segundo elemento y se guardan en index\_element y label\_element respectivamente, y se elimina el indice del array de indices. El siguiente paso recorrer el resto elementos del array de la característica. Ahora, toca comprobar si label\_element\_prior y label\_element o label\_element y la etiqueta del siguiente elemento son distintos, en ese caso se añade la partición actual al array de partes discretizadas, se crea un nuevo array para la parte actual y se incrementa en 1 la variable num\_part. A continuación independientemente del resultado de la comprobación anterior, se añade label\_element al array de la partición actual, se le coloca en el array de numpy \ref{numpy} de partición según indice se guardada en la posción establecida por index\_element el valor de num\_part, se guardan los valores de index\_element y label\_element en index\_element\_prior y label\_element\_prior respectivamente, y en index\_element y label\_element se guardan los valores del siguiente elemento, eliminado el indice de la lista indices. Una vez terminados todos los elementos toca añadir el ultimo elemento a la partición actual, actualizar el valor de posición del indice en el array de particiones de indices con el valor de num\_part y se añade la partición actual al array de partes discretizadas. A continuación, si quedan elementos en la lista de indices de añade al array de lista de particiones se incrementa en 1 la variable num\_part y se actualiza con este valor los indices del array de numpy \ref{numpy} de particiones de indices. Llegado a este punto solo nos queda calcular al entropía de la característica con la función calculateEntropy, a la cual se le pasa como parámetros en número de particiones, el array de numpy \ref{numpy} de partes de indices y el array de numpy \ref{numpy} con todos los indices de la base de datos y veremos ahora después, insertar en la colección discreeting los valores para esta característica, que son el nombre de la característica, en número de muestras en el que aparece el característica, la entropía de la característica, la discretización que se ha realizado y por ultimo el número de partición que se han realizado, cerrar el cursor, salir del bucle infinito con break y terminar eliminado la conexión con la base de datos.\\

La función calculateEntropy se utiliza para calcular la entropía aplicando el suavizado de Laplace, pudiendo calcularse de una características, de la clase o de una características conociendo la clase, eso solo depende de los parámetros que se le pasen. Los parámetros que se le tienen que pasar son los datos que a de ser un array de numpy, por ejemplo para la característica los datos serían el indice de la partición a la que corresponde el indice, en la clase sería la etiqueta que correspondiente al indice, y el número de valores distintos que forman los datos. Está consiste en crear un array de numpy \ref{numpy} de ceros con tamaño el número de particiones que se le pasan para almacenar los pasos de la entropía y una variable donde se guarda el numero total de elementos. Después, se recorre el array de la particiones aplicándole un enumerate para obtener el indice y el elemento del array de particiones. A continuación, se utiliza la función calculate\_num\_labels la cual crea un array de ones, mediante la cual se aplica el suavizado de Laplace, y se cuenta en el array de numpy \ref{numpy} el número de elementos de cada etiqueta y se devuelve éste. A continuación, se suman el número total de elementos del array de etiquetas y se añade al total elementos. El siguiente paso, consiste en dividir el array de etiquetas entre el número total de elementos que contiene este array para después multiplicar este número por el logaritmo de el mismo, terminado por sumar todos los valores y cambiandole el signo y se guarda en array de entropía en el indice de la partición. Una vez terminadas todos los elementos del array de partes se divide el array de entropía entre el total de elementos contados, sumando todos los valores y terminando por devolver este ultimo valor.\\

En este punto que ya se ha terminado el proceso de discretización, ahora nos toca realizar un proceso de selección filter, concretamente un proceso forward, para seleccionar las características que mayor información proporcionen a la hora de clasificar las muestras proporcionadas, por lo cual se implemento el método calculateRanking en el fichero selection\_filter.py. A este método se le pasa como parámetros el prefijo a utilizar para las colecciones, la dirección IP del servidor de base de datos MongoDB \ref{mongodb}, el nombre de la base de datos, la lista con los nombre utilizados para las colecciones, el número de características a seleccionar y el número total de muestras de la base de datos proporcionada.\\




\chapter{EXPERIMENTOS Y RESULTADOS}


\chapter{CONCLUSIONES Y PROPUESTAS}

\section{Conclusiones}


\section{Trabajo futuro}

4 Ngrams

Paralelizarción y Cluster

Microservicios

Orquestación Kubernetes

Optimización

\renewcommand{\refname}{BIBLIOGRAFÍA}
%\bibliographystyle{jmb}
%\bibliography{mibibliografia}

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,mibibliografia}

\addcontentsline{toc}{chapter}{BIBLIOGRAFIA}

%\chapter*{CONTENIDO DEL CD}
%En el contenido del CD que acompaña a la memoria podemos encontrar los
%siguientes recursos:
%\begin{itemize}
% \item Memoria del trabajo en los formatos PDF, DOCX y DOC dentro del directorio
%Memoria.
% \item Código fuente del trabajo dentro del directorio Código fuente.
% \item Libros y artículos a los que se ha hecho referencia durante la memoria y que se han
%utilizado como bibliografía. Los cuales podemos encontrar en el directorio
%Bibliografía.
% \item Páginas Web que han servido de bibliografía. Las podemos encontrar dentro del
%directorio Bibliografia/Enlaces Web.
% \item Manual de usuario de la aplicación junto con ejemplos, que podemos encontrar en
%el directorio Manual y ejemplos.
%\end{itemize}

%\addcontentsline{toc}{chapter}{CONTENIDO DEL CD}

%\appendix
%\chapter{EJEMPLO DE USO DE LA HERRAMIENTA X}
%
%
%\chapter{MANUAL DE USUARIO}
%
%\section{Introducción}
%
%
%\section{Pantalla de bienvenida}
%

\end{document}
